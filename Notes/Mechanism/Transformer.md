# Transformer: Multi-head Self-attention

> The dominant approach recently (2019)

Task: Machine Translation with parallel corpus => predict each translated word

## Background

### Motivation

* We want **parallelization** but RNNs (e.g. LSTM, GRU) are inherently sequential
* Despite GRUs and LSTMs, RNNs still need attention mechanism to *deal with long range dependencies* - **path length** between states grows with sequence

### Self-attention

> Can we *replace sequential computation (i.e. RNNs) entirely* with just self-attention?!

* It is really fast (you can do this very quickly on a GPU)

Attention is cheap: (Amount of FLOPs)

| Mechnism       | Complexity                                                      |
| -------------- | --------------------------------------------------------------- |
| Self-attention | $O(\text{length}^2 \cdot \text{dim})$                           |
| RNN (LSTM)     | $O(\text{length} \cdot \text{dim}^2)$                           |
| Convolution    | $O(\text{length} \cdot \text{dim}^2 \cdot \text{kernel width})$ |

> Can we *simulate convolution* with multi-head?!

* with more heads
* or heads are function of positions

#### Dot-Project Attention

$$
\textit{Attention}(Q, K, V) = \textit{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

Problem: As $d_k$ get large, the variance of $q^Tk$ increses $\rightarrow$ some values inside the softmax get larger $\rightarrow$ the softmax gets very peaked $\rightarrow$ hence its gradient gets smaller

Solution: **Scale** by length of query/key vectors - $\times \frac{1}{\sqrt{d_k}}$

#### Multi-head Attention

> Problem with simple self-attention: Only *one way* for words to *interact* with one-another

$$
\textit{MultiHead}(Q, K, V) = \textit{concat}(head_1, \dots, head_h)W^O
$$

where $head_i = \textit{Attention}(QW_i^Q, KW_i^K, VW_i^K)$

## Model

* non-recurrent sequence-to-sequence encoder-decoder
* a multi-head attention (self-attention) stack
* final cost/error function is standard cross-entropy error on top of a softmax classifier

### A Transformer Block

Each block has two "sublayers"

1. Multi-head Attention
2. 2-layer Feed-forward Neural Net (with ReLU)

Each of these two step also has:

* Residual (short-circuit) connection and [LayerNorm](https://arxiv.org/pdf/1607.06450.pdf)
* $\operatorname{LayerNorm}(x + \operatorname{Sublayer}(x))$
  * LayerNorm changes input to have mean 0 and variance 1
    * per layer and per training point
    * adds two more parameter

### Encoder

* Actual word representations are byte-pair encodings
* Also added is a **positional encoding** so same words at different locations have different overall representations

$$
PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}}) \\
$$

### Decoder

mimic language model: (it can't look forward because it's illegal)

* the causal self-attention
* impose causality by just mask out the positions that you can look at

Masked decoder self-attention on previously generated outputs:

1. Encoder-Decoder Attention where **queries** come from previous decoder layer
2. And **keys** and **values** come from output of encoder

## Conclusion

### Tips and tricks of the Transformer

* Byte-pair encodings
* Checkpoint averaging
* Adam optimizer with learning rate changes
* Dropout during training at every layer just before adding residual
* Label smoothing
* Auto-regressive decoding with bea search and length penalties

> Use of transformers is sperading but they are *hard to optimize* and unlike LSTMs don't usually just work out of the box and they don't play well yet with other building blocks on tasks.

## Resources

* [**Harvard NLP - The Annotated Transformer**](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - Must read!
* [Stanford CS224n Lecture14 Transformers Slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf)
* [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)
* [PyTorch Transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)
  * [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html)

### Tutorial

* [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 â€“ Transformers and Self-Attention - YouTube](https://www.youtube.com/watch?v=5vcj8kSwBCY)
* [Youtube - Attention Is All You Need](https://youtu.be/iDulhoQ2pro) - TODO
* [**The Illustrated Transformer**](https://jalammar.github.io/illustrated-transformer/)
* [bilibili - Transformer Explain I](https://www.bilibili.com/video/av58239477)
* [bilibili - Transfermer BERT pre-train II](https://www.bilibili.com/video/av60168891)

### Github

* [**Transformers**](https://huggingface.co/transformers/index.html)
  * [huggingface/transformers: ðŸ¤— Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.](https://github.com/huggingface/transformers)
* [Kyubyong/transformer](https://github.com/Kyubyong/transformer) - A TensorFlow Implementation of the Transformer: Attention Is All You Need
* [lena-voita/the-story-of-heads](https://github.com/lena-voita/the-story-of-heads) - This is a repository with the code for the ACL 2019 paper "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
